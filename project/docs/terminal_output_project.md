# Project

## Overview
This document contains the recorded terminal output generated by running
the implementation of **Telco customer churn project**.

## Execution Details
- **Language:** Python 3.12
- **Virtual Environment:** `fisEnv`
- **Script:** `project/src/project.py`
- **Working Directory:** `FIS4041_F_40121723_40115563`

### Command Used
```bash
python project/src/project.py
```

## Terminal Output
```
--- Step 1: Data Introduction & Loading ---
Data Loaded Successfully.

Shape of dataset: (7043, 33)

Data Types:
str        23
int64       6
float64     3
object      1
Name: count, dtype: int64

Target Distribution (Churn Value):
Churn Value
0    0.73463
1    0.26537
Name: proportion, dtype: float64

--- Step 2: Data Preprocessing & Effect Analysis ---

--- Step 3: Dimensionality Reduction (PCA) & Cost-Benefit Analysis ---
Original Features: 30
Features after PCA (95% Variance): 17

Running quick comparison: PCA vs. No-PCA (using Random Forest)...
Method          | Training Time (s)  | Accuracy  
--------------------------------------------------
Original        | 0.1230             | 0.7956
With PCA        | 0.1096             | 0.7963

--- Step 4 & 5: Model Selection (Bagging vs Boosting) & Hyperparameter Tuning ---

Training Random Forest (Bagging) with GridSearchCV...
Fitting 5 folds for each of 12 candidates, totalling 60 fits
Best Params for Random Forest (Bagging): {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50}
Best CV F1-Score: 0.6032

Training Gradient Boosting (Boosting) with GridSearchCV...
Fitting 5 folds for each of 8 candidates, totalling 40 fits
Best Params for Gradient Boosting (Boosting): {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}
Best CV F1-Score: 0.5999

--- Step 6 & 7: Final Evaluation, Reproducibility & Visualization ---

==================== Random Forest (Bagging) Evaluation ====================
              precision    recall  f1-score   support

           0       0.85      0.91      0.88      1035
           1       0.68      0.55      0.61       374

    accuracy                           0.81      1409
   macro avg       0.76      0.73      0.74      1409
weighted avg       0.80      0.81      0.80      1409


==================== Gradient Boosting (Boosting) Evaluation ====================
              precision    recall  f1-score   support

           0       0.84      0.90      0.87      1035
           1       0.66      0.53      0.59       374

    accuracy                           0.80      1409
   macro avg       0.75      0.72      0.73      1409
weighted avg       0.79      0.80      0.80      1409

Generating Learning Curves for Gradient Boosting (Best Model Analysis)...
```

## Visualizations

### Visualization of preprocessing effect 
![Visualization of preprocessing effect](../images/Visualization_of_Preprocessing_Effect.png)

### Confusion Matrix and ROC Curve comparison
![Confusion Matrix and ROC Curve comparison](../images/Confusion_Matrix_and_ROC_Curve_Comparison.png)

### Learning Curve for Gradient Boosting
![Learning Curve (Gradient Boosting)](../images/Learning_Curve_(Gradient_Boosting).png)